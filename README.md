# ğŸ¤– RequÃªter des fichiers PDF en local grÃ¢ce Ã  Ollama + LangChain

Cette application locale RAG (Retrieval Augmented Generation) permet de discuter avec vos documents PDF grÃ¢ce Ã  Ollama et LangChain. 

Ce projet comprend un notebook Jupyter pour l'expÃ©rimentation 

et une interface web Streamlit pour une interaction simplifiÃ©e.

[![Tests Python](https://github.com/tonykipkemboi/ollama_pdf_rag/actions/workflows/tests.yml/badge.svg)](https://github.com/tonykipkemboi/ollama_pdf_rag/actions/workflows/tests.yml)

## Structure du projet
```
ollama_pdf_rag/
â”œâ”€â”€ src/ # Code source
â”‚ â”œâ”€â”€ app/ # Application Streamlit
â”‚ â”‚ â”œâ”€â”€ components/ # Composants d'interface utilisateur
â”‚ â”‚ â”‚ â”œâ”€â”€ chat.py # Interface de chat
â”‚ â”‚ â”‚ â”œâ”€â”€ pdf_viewer.py # Affichage PDF
â”‚ â”‚ â”‚ â””â”€â”€ sidebar.py # ContrÃ´les de la barre latÃ©rale
â”‚ â”‚ â””â”€â”€ main.py # Application principale
â”‚ â””â”€â”€ core/ # FonctionnalitÃ©s principales
â”‚ â”œâ”€â”€ document.py # Traitement des documents
â”‚ â”œâ”€â”€ embeddings.py # Incorporations vectorielles
â”‚ â”œâ”€â”€ llm.py # Configuration LLM
â”‚ â””â”€â”€ rag.py # Pipeline RAG
â”œâ”€â”€ data/ # Stockage de donnÃ©es
â”‚ â”œâ”€â”€ pdfs/ # Stockage PDF
â”‚ â”‚ â””â”€â”€ sample/ # Exemples de PDF
â”‚ â””â”€â”€ vectors/ # Stockage de base de donnÃ©es vectorielles
â”œâ”€â”€ notebooks/ # Bloc-notes Jupyter
â”‚ â””â”€â”€ experiments/ # Bloc-notes expÃ©rimentaux
â”œâ”€â”€ tests/ # Tests unitaires
â”œâ”€â”€ docs/ # Documentation
â””â”€â”€ run.py # ExÃ©cuteur d'application
```

## ğŸ“º Tutoriel vidÃ©o
<a href="https://youtu.be/ztBJqzBU5kc">
<img src="https://img.youtube.com/vi/ztBJqzBU5kc/hqdefault.jpg" alt="Regarder la vidÃ©o" width="100%">
</a>

## âœ¨ FonctionnalitÃ©s

- ğŸ”’ Traitement entiÃ¨rement localÂ : aucune donnÃ©e ne quitte votre machine
- ğŸ“„ Traitement PDF avec segmentation intelligente
- ğŸ§  RÃ©cupÃ©ration multi-requÃªtes pour une meilleure comprÃ©hension du contexte
- ğŸ¯ ImplÃ©mentation RAG avancÃ©e avec LangChain
- ğŸ–¥ï¸ Interface Streamlit Ã©purÃ©e
- ğŸ““ Bloc-notes Jupyter pour expÃ©rimentation

## ğŸš€ Premiers pas

### PrÃ©requis

1. **Installer Ollama**
- Visitez le site web d'Ollama (https://ollama.ai) pour tÃ©lÃ©charger et installer
- Extraire les modÃ¨les requisÂ :
```bash
ollama pull llama3.2 # ou votre modÃ¨le prÃ©fÃ©rÃ©
ollama pull nomic-embed-text
```

2. **Cloner le dÃ©pÃ´t**
```bash
git clone https://github.com/tonykipkemboi/ollama_pdf_rag.git
cd ollama_pdf_rag
```

3. **Configurer l'environnement**
```bash
python -m venv venv
source venv/bin/activate # Sous WindowsÂ : .\venv\Scripts\activate
pip install -r requirements.txt
```

ClÃ© DÃ©pendances et leurs versionsÂ :
```txt
ollama==0.4.4
streamlit==1.40.0
pdfplumber==0.11.4
langchain==0.1.20
langchain-core==0.1.53
langchain-ollama==0.0.2
chromadb==0.4.22
```

### ğŸ® ExÃ©cution de l'application

#### Option 1Â : Interface Streamlit
```bash
python run.py
```
Ouvrez ensuite votre navigateur Ã  l'adresse `http://localhost:8501`

![Interface utilisateur Streamlit](st_app_ui.png)
*Interface Streamlit affichant la visionneuse PDF et les fonctionnalitÃ©s de chat*

#### Option 2Â : Bloc-notes Jupyter
```bash
bloc-notes Jupyter
```
Ouvrez `updated_rag_notebook.ipynb` pour tester Le code

## ğŸ’¡ Conseils d'utilisation

1. **TÃ©lÃ©charger un PDF**Â : Utilisez l'outil de tÃ©lÃ©chargement de fichiers dans l'interface Streamlit ou essayez l'exemple de PDF
2. **SÃ©lectionner un modÃ¨le**Â : Choisissez parmi les modÃ¨les Ollama disponibles localement
3. **Poser des questions**Â : Commencez Ã  discuter avec votre PDF via l'interface de discussion
4. **Ajuster l'affichage**Â : Utilisez le curseur de zoom pour ajuster la visibilitÃ© du PDF
5. **Nettoyer**Â : Utilisez le bouton Â«Â Supprimer la collectionÂ Â» lorsque vous changez de document

## ğŸ¤ Contribuer

N'hÃ©sitez pas Ã Â :
- Ouvrir des problÃ¨mes pour signaler des bugs ou des suggestions
- Soumettre des demandes d'extraction
- Commenter la vidÃ©o YouTube pour poser des questions
- Ajouter une Ã©toile au dÃ©pÃ´t si vous le trouvez utileÂ !

## âš ï¸ DÃ©pannage

- Assurez-vous qu'Ollama s'exÃ©cute en arriÃ¨re-plan
- VÃ©rifiez que les modÃ¨les requis sont tÃ©lÃ©chargÃ©s
- VÃ©rifiez que l'environnement Python est activÃ©
- Pour les utilisateurs Windows, assurez-vous que WSL2 est correctement configurÃ© si vous utilisez Ollama

### Erreurs courantes

#### Erreur DLL ONNX
Si vous rencontrez cette erreurÂ :
```
Ã‰chec du chargement de la DLL lors de l'importation de onnx_copy2py_exportÂ : une routine d'initialisation de la bibliothÃ¨que de liens dynamiques (DLL) a Ã©chouÃ©.
```

Essayez ces solutionsÂ :
1. Installez Microsoft Visual C++ RedistributableÂ :
- TÃ©lÃ©chargez et installez les versions x64 et x86 depuis le site web officiel de Microsoft (https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist)
- RedÃ©marrez votre ordinateur aprÃ¨s l'installation

2. Si l'erreur persiste, essayez d'installer ONNX Runtime manuellementÂ :
```bash
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime
```
#### SystÃ¨mes CPU uniquement
Si vous utilisez un systÃ¨me CPU uniquementÂ :

1. Assurez-vous de disposer de la version CPU d'ONNX RuntimeÂ :
```bash
pip uninstall onnxruntime-gpu # Supprimer la version GPU si installÃ©e
pip install onnxruntime # Installer la version CPU uniquement
```

2. Vous devrez peut-Ãªtre modifier la taille des blocs dans le code pour Ã©viter les problÃ¨mes de mÃ©moireÂ :
- RÃ©duisez Â«Â chunk_sizeÂ Â» Ã  500-1000 si vous rencontrez des problÃ¨mes de mÃ©moire
- Augmentez Â«Â chunk_overlapÂ Â» pour une meilleure prÃ©servation du contexte

RemarqueÂ : L'application sera plus lente sur les systÃ¨mes CPU uniquement, mais fonctionnera toujours efficacement.

## ğŸ§ª Tests

### ExÃ©cution des tests
```bash
# ExÃ©cution de tous les tests
python -m unittest discover tests

# ExÃ©cution dÃ©taillÃ©e des tests
python -m unittest discover tests -v
```

### Hooks prÃ©-commit
Le projet utilise des hooks prÃ©-commit pour garantir la qualitÃ© du code. Pour configurerÂ :

```bash
pip install pre-commit
pre-commit install
```

Cela permetÂ :
- d'exÃ©cuter des tests avant chaque commit
- d'effectuer des vÃ©rifications linting
- de garantir le respect des normes de qualitÃ© du code

### IntÃ©gration continue
Le projet utilise GitHub Actions pour l'intÃ©gration continue. Ã€ chaque requÃªte push et pullÂ :
- Les tests sont exÃ©cutÃ©s sur plusieurs versions de Python (3.9, 3.10, 3.11)
- Les dÃ©pendances sont installÃ©es
- Les modÃ¨les Ollama sont extraits
- Les rÃ©sultats des tests sont tÃ©lÃ©chargÃ©s sous forme d'artefacts

## ğŸ“ Licence

Ce projet est open source et disponible sous licence MIT.

---

## â­ï¸ Histoire des Ã©toiles

[![Graphique de l'histoire des Ã©toiles](https://api.star-history.com/svg?repos=tonykipkemboi/ollama_pdf_rag&type=Date)](https://star-history.com/#tonykipkemboi/ollama_pdf_rag&Date)

CrÃ©Ã© avec â¤ï¸ par [Tony Kipkemboi!](https://tonykipkemboi.com)
